# Цепи Маркова
Простейшая наивная реализация цепей Марков для генерации текста.

## Нормализация
Для начала вам нужно собрать датасет. Им может быть любой связный текст, будь-то литературное произведение или выгруженные сообщения из вашего чата). Создайте файл `text.txt` и вставьте туда свой датасет, после этого запустите код из файла `normalize.py` через команду `python normalize.py`.

### Что произойдет?
Код в файле `normalize.py` очистит текст от ненужных символов и разобьет его на предложения, переведя их в нижний регистр. После этого создаст файл `text_normalized.txt`. Этот файл будет содержать предложения с новой строки в нижнем регистре.  


## Обучение
**Далее нужно "обучить" модель** (конечно, никакого обучения по факту здесь нет). Запустите файл `learn.py`: `python learn.py`.  

### Что произойдет?
Алгоритм будет брать каждое отдельное предложение и разбивать его на слова. После этого итерироваться **попарно** по полученным словам. При этом составляя словарь, содержащий соответствие слов от предыдущих. Например:
```json
{
    "привет": [
        "мир",
        "как"
    ],
    "как": [
        "жизнь",
        "дела"
    ]
}
```  
и так далее. Затем, обученная модель сохраняется в файл `model.json` для дальнейшего использования.  

## Использование
После выполнения всех предыдущих этапов вы получили обученную модель `model.json`, которую можно использовать в продакшене.  
Запустите файл `main.py`: `python main.py`.

### Что произойдет?
Перво-наперво код загрузит модель из файла. После этого он выберет случайный элемент из модели и добавит его ключ и значение в массив сгенерируемого текста (назовем его `generated`): `[key, value]`. Далее, алгоритм входит в цикл, в котором происходит несколько проверок:

1. Если последний элемент массива `generated` существует как ключ в модели, то получаем из модели случайное значение из массива, который располагается по этому ключу. После этого добавляем в конец массива `generated`.
2. Если последний элемент не найден как ключ в модели, а так же алгоритму разрешено отклонение от контекста (второй аргумент в функции `generate`), то производится попытка подбора слова ко второму с конца слову из массива `generated`, потом третьего и т.д.
3. Если последний элемент не найден как ключ в модели и алгоритму запрещено отклонятся от контекста, то цикл преждевременно прекращается, возвращая текущий результат.

Далее массив слов объединяется в строку через пробел и возвращается функцией.

# Почему это не будет работать?
Этот алгоритм – это мое видение цепей Маркова и то, как я их понимаю (возможно не понимаю). Сам по себе алгоритм не может быть использован для генерации осмысленного текста (насколько бы большим датасет не был) просто потому что он не способен распознавать и понимать контекст. Он всего лишь генерирует случайный текст, основываясь на предыдущем в предложении слове (очевидно, что такой метод очень наивен).

---
**by AndcoolSystems, 26 February 2025**